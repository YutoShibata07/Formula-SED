<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Formula-Supervised Sound Event Detection: Pre-Training Without Real Data</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Formula-Supervised Sound Event Detection: Pre-Training Without Real Data</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yutoshibata07.github.io/YutoShibata/" target="_blank">Yuto Shibata</a>,</span>
                <span class="author-block">
                  <a href="https://sites.google.com/view/keitarotanaka/" target="_blank">Keitaro Tanaka</a>,</span>
                  <span class="author-block">
                    <a href="https://www.ybando.jp/" target="_blank">Yoshiaki Bando</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://sites.google.com/site/ksukeimotoj/" target="_blank">Keisuke Imoto</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://hirokatsukataoka.net/" target="_blank">Hirokatsu Kataoka</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://www.st.keio.ac.jp/en/tprofile/elec/aoki.html" target="_blank">Yoshimitsu Aoki</a>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Keio University, AIST, Waseda University, Doshisha University<br>ICASSP2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10888414" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/data_comparison_0331.png" alt="Formula-SED" class="center-image" style="max-width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        We create a synthetic dataset for SED, named Formula-SED, and propose a novel formula-driven pre-training method that uses acoustic synthesis parameters as labels with correct timestamps.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we propose a novel framework for pre-training environmental sound analysis models by utilizing parametrically synthesized acoustic signals using formula-driven methods. Specifically, we outline detailed procedures and evaluate their effectiveness for Sound Event Detection (SED). The SED task, which involves estimating the types and timings of sound events, is particularly challenged by the difficulty of acquiring a sufficient quantity of accurately labeled training data. Moreover, it is well known that manually annotated labels often contain noises and are significantly influenced by the subjective judgment of annotators. To address these challenges, we propose a novel pre-training method that utilizes a synthetic dataset, Formula-SED, where acoustic data are generated solely based on mathematical formulas. The proposed method enables large-scale pre-training by using the synthesis parameters applied at each time step as ground truth labels, thereby eliminating label noise and bias. We demonstrate that large-scale pre-training with Formula-SED significantly enhances model accuracy and accelerates training, as evidenced by our results in the DESED dataset used for DCASE2023 Challenge Task 4. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- サンプル音声セクション -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Sample Audio Clips</h2>
    <div class="columns is-multiline">

      <!-- 1つ目の音声 -->
      <div class="column is-one-third has-text-centered">
        <img src="static/images/formula-spec-0.png" alt="Spectrogram 1" style="max-width: 100%;">
        <p>Example Audio #1</p>
        <audio controls>
          <source src="static/audio/formula-0.wav" type="audio/wav">
          <!-- ここに mp3 を併記しておくと、wav に対応していないブラウザでも再生可能にできます -->
          <!-- <source src="static/audio/sample1.mp3" type="audio/mp3"> -->
          Your browser does not support the audio element.
        </audio>
      </div>

      <!-- 2つ目の音声 -->
      <div class="column is-one-third has-text-centered">
        <img src="static/images/formula-spec-1.png" alt="Spectrogram 2" style="max-width: 100%;">
        <p>Example Audio #2</p>
        <audio controls>
          <source src="static/audio/formula-1.wav" type="audio/wav">
          <!-- ここに mp3 を併記しておくと、wav に対応していないブラウザでも再生可能にできます -->
          <!-- <source src="static/audio/sample2.mp3" type="audio/mp3"> -->
          Your browser does not support the audio element.
        </audio>
      </div>
      <!-- 3つ目の音声 -->
      <div class="column is-one-third has-text-centered">
        <img src="static/images/formula-spec-2.png" alt="Spectrogram 3" style="max-width: 100%;">
        <p>Example Audio #3</p>
        <audio controls>
          <source src="static/audio/formula-2.wav" type="audio/wav">
          <!-- ここに mp3 を併記しておくと、wav に対応していないブラウザでも再生可能にできます -->
          <!-- <source src="static/audio/sample3.mp3" type="audio/mp3"> -->
          Your browser does not support the audio element.
        </audio>
      </div>
      <!-- 4つ目の音声 -->
      <div class="column is-one-third has-text-centered">
        <img src="static/images/formula-spec-3.png" alt="Spectrogram 4" style="max-width: 100%;">
        <p>Example Audio #4</p>
        <audio controls>
          <source src="static/audio/formula-3.wav" type="audio/wav">
          <!-- ここに mp3 を併記しておくと、wav に対応していないブラウザでも再生可能にできます -->
          <!-- <source src="static/audio/sample4.mp3" type="audio/mp3"> -->
          Your browser does not support the audio element.
        </audio>
      </div>
      <!-- 5つ目の音声 -->
      <div class="column is-one-third has-text-centered">
        <img src="static/images/formula-spec-4.png" alt="Spectrogram 5" style="max-width: 100%;">
        <p>Example Audio #5</p>
        <audio controls>
          <source src="static/audio/formula-4.wav" type="audio/wav">
          <!-- ここに mp3 を併記しておくと、wav に対応していないブラウザでも再生可能にできます -->
          <!-- <source src="static/audio/sample5.mp3" type="audio/mp3"> -->
          Your browser does not support the audio element.
        </audio>
      </div>
      <!-- 6つ目の音声 -->
      <div class="column is-one-third has-text-centered">
        <img src="static/images/formula-spec-5.png" alt="Spectrogram 6" style="max-width: 100%;">
        <p>Example Audio #6</p>
        <audio controls>
          <source src="static/audio/formula-5.wav" type="audio/wav">
          <!-- ここに mp3 を併記しておくと、wav に対応していないブラウザでも再生可能にできます -->
          <!-- <source src="static/audio/sample6.mp3" type="audio/mp3"> -->
          Your browser does not support the audio element.
        </audio>
      </div>

    </div>
  </div>
</section>

<!-- overviewを画像を用いて説明 -->
<section class ="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Overview</h2>
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <img src="static/images/ICASSP_teaser_cameraready.png" alt="Overview" style="max-width: 100%;">
        <p>Overview of the proposed method. We effectively pre-train SED models using acoustic data generated solely based on mathematical formulas.</p>
      </div>
    </div>
  </div>

<!-- 事前学習によってモデルの精度が向上したのでその学習曲線も載せる -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Learning Curve</h2>
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <img src="static/images/training_curve.png" alt="Downstream training curve for baseline CRNN" style="max-width: 100%;">
        <p>Downstream training curve for baseline CRNN. </p>
        <p>We can see that the model with formula-driven pre-training converges faster and achieves better performance.</p>
      </div>
    </div>
  </div>


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/images/ICASSP2025_Poster_0331.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@INPROCEEDINGS{10888414,
        author={Shibata, Yuto and Tanaka, Keitaro and Bando, Yoshiaki and Imoto, Keisuke and Kalaoka, Hirokatsu and Aoki, Yoshimitsu},
        booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
        title={Formula-Supervised Sound Event Detection: Pre-Training Without Real Data}, 
        year={2025},
        volume={},
        number={},
        pages={1-5},
        keywords={Training;Accuracy;Event detection;Noise;Supervised learning;Training data;Acoustics;Mathematical models;Timing;Synthetic data;sound event detection;pre-training without real data;environmental sound synthesis},
        doi={10.1109/ICASSP49660.2025.10888414}}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
